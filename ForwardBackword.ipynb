{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9253132d-7d7d-42b0-9ff7-f090f84b31e1",
   "metadata": {},
   "source": [
    "# Ans : 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63dafc3-4840-426a-89ba-26d46b23cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Forward propagation is the process of passing input data through the network layers to generate an output. Its purpose is to compute the \n",
    "predicted output based on the current state of the network parameters (weights and biases).\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66719925-d83a-48ad-9b1d-d64a965efd69",
   "metadata": {},
   "source": [
    "# Ans : 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19673f-24a4-4ccd-b7b4-c4199cf5fb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input to Hidden Layer: For input x, the output of the hidden layer h is calculated using the weights ùëä and biases ùëè:\n",
    "\n",
    "h=f(Wx+b)\n",
    "\n",
    "where f is the activation function.\n",
    "\n",
    "Hidden Layer to Output Layer: If there is an output layer, the final output y is calculated as:\n",
    "\n",
    "y=g(Vh+c)\n",
    "where \n",
    "\n",
    "V and c are the weights and biases of the output layer, and g is the activation function of the output layer.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f3ce2-3ffc-4333-8d1e-b4e72e457425",
   "metadata": {},
   "source": [
    "# Ans : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f21ae1-4e2c-4732-a673-f9556c1a6ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Activation functions are applied to the weighted sum of inputs plus biases at each layer to introduce non-linearity into the network.\n",
    "This allows the network to learn complex patterns and representations. The activation function transforms the linear input into a non-linear\n",
    "output, which is then passed to the next layer.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6963d42-867a-4762-b8e4-9179a3099101",
   "metadata": {},
   "source": [
    "# Ans : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce3beb-db19-4946-a86f-ac13f74ed701",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Weights and biases are the parameters of the neural network that are learned during training:\n",
    "\n",
    "Weights: Determine the strength and direction of the input signals.\n",
    "Biases: Allow the activation function to be shifted to better fit the data. They provide the network with the ability to adjust the output along with the weights\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269934ad-2437-41d4-9a5b-a4f204f9fa38",
   "metadata": {},
   "source": [
    "# Ans : 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19bf8f2-af38-4a5a-94cb-76ade0b65456",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The softmax function converts the raw output scores from the network into a probability distribution over the predicted classes.\n",
    "This is particularly useful in multi-class classification problems, as it allows the network to assign a probability to each class, \n",
    "ensuring that the sum of all probabilities is 1.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cda76a-dcbb-4c2d-ae57-7bd904fcfc19",
   "metadata": {},
   "source": [
    "# Ans : 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcc5407-0601-4482-aae0-f3904adbdc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Backward propagation (backpropagation) is the process of calculating the gradient of the loss function with respect to each weight and bias\n",
    "in the network. These gradients are then used to update the network parameters to minimize the loss function, thereby training the network.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b4e247-0598-457e-85bb-9b82b14fe7ca",
   "metadata": {},
   "source": [
    "# Ans : 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28838a40-f619-45cb-abe6-2a9693eeede2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47de2002-c9d6-4628-a2b9-5904853ccdef",
   "metadata": {},
   "source": [
    "# Ans : 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442225d-f895-4adb-bcc9-b9fafbe08fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The chain rule is a fundamental calculus principle used to compute the derivative of a composition of functions. In the context of backpropagation, it is used to compute the gradients of the loss function with respect to each weight and bias in the network. By applying the chain rule, we can propagate the error backward through the network, layer by layer, to calculate the gradients efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccb177-4c69-4bac-a63a-6a36da57664f",
   "metadata": {},
   "source": [
    "# Ans : 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd35c25-8964-47f1-bce3-e45cec8f03ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ec196-be98-49e9-9657-10791bbef8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
